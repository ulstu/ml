{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 07. Нейронные сети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача Нелинейной Классификации\n",
    "Когда мы хотим создать классификатор изображений автомобилей при помощи машинного обучения, нам нужен тренировочный датасет с ПРАВИЛЬНЫМИ метками, автомобиль это или нет. После обучения мы получим хороший классификатор. Когда мы тестируем его с новым изображением, он ответит, изображен автомобиль на изображении или нет.\n",
    "![](../../img/lec07_01.png)\n",
    "\n",
    "Перед тем, как пойти дальше, нам необходимо понять, как комьпютер \"видит\" изображение. Как можно видеть в картинке справа, компьютер всегда \"видит\" изображение как набор пикселей со значениями интенсивности. Например, картинка ниже показывает позицию пикселя (оранжевая точка) и значение его интенсивности - 69. \n",
    "\n",
    "![](../../img/lec07_02.png)\n",
    "\n",
    "Теперь, давайте взглянем на процесс обучения. Во-первых, рассмотрим 2 пикселя как признаки.\n",
    "\n",
    "![](../../img/lec07_03.png)\n",
    "\n",
    "Во-вторых, определим **нелинейноую логистическую регрессию** как гипотезу ***H***. Наша цель состоит в том, чтобы найти подходящую ***H***, которая может отличать как положительные данные, так и отрицателньые.\n",
    "\n",
    "![](../../img/lec07_04.png)\n",
    "\n",
    "Наконец, получим все параметры ***H***. Сравним их с линейной логистической регрессией, нелинейная форма более сложная, поскольку содержит множество полиномиальных выражений.\n",
    "\n",
    "Однако, когда количество признаков большое, вышеуказанное решение не является оптимальным выбором для обучения нелинейной гипотезы. Предположим, что у нас есть изображение размером 50х50 пикселей, и все пиксели являются признаками, следовательно, нелинейная гипотеза должна содержать более 2500 признаков, поскольку у ***H*** есть дополнительные квадратичные или кубические признаки. Такие вычисления, с целью найти все параметры $\\theta$ (или $W$) этих признаков, обходятся очень дорого в расчете на обучающие данные. \n",
    "\n",
    "По этой причине нам неоходим более эффективный способ - Нейронная Сеть, которая является очень мощной и широко используемой моделью для обучения сложных нелинейных гипотез во многих приложениях.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная Сеть (NN)\n",
    "\n",
    "В данном разделе, мы поговорим о том, как представить гипотезу, используя нейронные сети.\n",
    "\n",
    "Изначально, Нейронная Сеть - это алгоритм, вдохновленный человеческим мозгом, алгоритм, который пытается мимикрировать под человеческий мозг. Как и нейроны человеческого мозга, NN подразумевает множество взаимосвязанных узлов (т.н. нейронов), организованных в слои.\n",
    "\n",
    "### Простейшая Нейронная Сеть\n",
    "\n",
    "Простейшая NN модель содержит лишь один нейрон. **Мы можем рассматривать нейрон (узел) как логистическую единицу с\n",
    "*(Логистической) Сигмоидальной Функцией Активации****, выходное значение которой основано на сигмоидальной функции активации. \n",
    "\n",
    "* Параметры $\\theta$ * в терминологии ** NN ** называют **весами** ***weights***.\n",
    "* В зависимости от задачи, вы можете сами решить, использовать единицы смещения или нет.\n",
    "\n",
    "![](../../img/lec07_05.png)\n",
    "\n",
    "### Нейронная Сеть (NN)\n",
    "\n",
    "* Слой 1 называют **Входным Слоем**. Он принимает признаки.\n",
    "* Последний слой назван **Выходным Слоем**. Он выводящит итоговое значение гипотезы ***H***.\n",
    "* Слой между Входным и Выходным называют **Скрытый Слой**, который является блоком, где мы группируем нейроны вметсе.\n",
    "\n",
    "![](../../img/lec07_06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Примечание\n",
    "\n",
    "Нейрон (узел) - это по сути логистическая единица с Сигмоидальной Функцией Активации (прим. простая логистическая регрессия). Если вы уже встречались с Линейной Алгеброй, вы можете представить скрытый слой как линейную комбинацию узлов предыдущих слоев. **Поэтому, главная идея NN заключается в решении сложных задач нелинейной классификации путем использования множества последовательностей простых логистических регрессий.** \n",
    "\n",
    "С целью получения понятного изображения о том, что делает данная нейронная сеть, давайте рассмотрим шаги вычислений и визуализируем их.\n",
    "\n",
    "**Во-первых**, мы визуализируем процесс преобразования матрицы $\\theta$, контролирующей отображением функций из слоя *j* в *j+1.*\n",
    "\n",
    "![](../../img/lec07_07.png)\n",
    "![](../../img/lec07_08.png)\n",
    "\n",
    "**Во-вторых**, мы визуализируем каждый вычислительных процесс нейронов. Заметьте, что выходное значение каждого нейрона вычисленно при помощи сигмоидальной функции активации.\n",
    "\n",
    "![](../../img/lec07_09.png)\n",
    "![](../../img/lec07_10.png)\n",
    "![](../../img/lec07_11.png)\n",
    "![](../../img/lec07_12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Архитектуры иных Нейронных Сетей\n",
    "\n",
    "Архитектуры иных Нейронных Сетей могут быть разработаны путем расширения скрытых слоев. Количество нейронов на слой основывается на задаче.\n",
    "![](../../img/lec07_13.png)\n",
    "\n",
    "### Приложения и Примеры\n",
    "\n",
    "Основываясь на вышеуказанной концепции, мы разработаем NN, чтобы показать, как модель NN может быть применена для задач нелинейной классификации.\n",
    "\n",
    "#### Пример1 — \"И\" (AND)\n",
    "\n",
    "Мы можем разработать простую NN с одним нейроном для решения проблемы AND. Даны -30, 20 и 20 как весы, можно определить *сигмоидальную функцию активации* ***H*** данного нейрона (узла). При прогнозировании данных, используя ***H,***, получаем отличный результат.\n",
    "\n",
    "\n",
    "![](../../img/lec07_14.png)\n",
    "![](../../img/lec07_15.png)\n",
    "\n",
    "\n",
    "#### Пример2 — \"ИЛИ\" (OR)\n",
    "\n",
    "Концепция операции OR аналогичен AND, но вес единицы смещения изменяем на -10.\n",
    "\n",
    "![](../../img/lec07_16.png)\n",
    "![](../../img/lec07_17.png)\n",
    "\n",
    "#### Пример3 — \"Отрицание\" (Negation)\n",
    "![](../../img/lec07_18.png)\n",
    "\n",
    "#### Пример4 — \"И-НЕ\" NAND\n",
    "![](../../img/lec07_19.png)\n",
    "\n",
    "#### Пример5 — \"Исключающее ИЛИ\" - XOR\n",
    "\n",
    "Ясно, что это задача нелинейной классификации, следовательно, мы может решить ее, используя нелинейную логистическую регрессию ***H***, о которой мы говорли в начале.  \n",
    "![](../../img/lec07_20.png)\n",
    "![](../../img/lec07_21.png)\n",
    "\n",
    "Однако, когда количество признаков и данных большое, ***H*** будет слишком усложнена для понимания и стоимость вычислений будет слишком высока.\n",
    "\n",
    "**Вместо этого, мы используем структуру NN, чтобы сделать модель *H* более понятной и простой.** Здесь мы ****** применяем NN к задаче XOR, основывающуюся на AND, NAND and OR.\n",
    "![](../../img/lec07_22.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Задача Многоклассовой Классификации\n",
    "\n",
    "В данном случае, мы можем добавить узлы в Выходной Слой. Каждый узел может прогнозировать один класс, концепция схожа с механизмом один-против-всех.\n",
    "\n",
    "![](../../img/lec07_23.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прямое Распространение (Forward Propagation )\n",
    "\n",
    "Шаги описываются ниже:\n",
    "\n",
    "* Во-первых, нам необходима матрица весов $\\theta$ (обозначенная W) для каждого скрытого слоя. Мы можем получить их случайным образом или по предварительному знанию.  \n",
    "* Начиная со Входного Слоя X(обозанченного $a^1$).\n",
    "* Производим Прямое Распространение через Выходной Слой и получаем итоговое значение.\n",
    "* Используем итоговое значение для прогнозирования.\n",
    "\n",
    "![](../../img/lec07_24.png)\n",
    "\n",
    "\n",
    "### Подробности\n",
    "\n",
    "Давайте возьмем за пример задачу бинарной классификации и визуализируем подробности Прямого Распространения. В первую очередь, нам необходима завершенная архитектура NN, то есть, все веса матрицы **W** уже известны.\n",
    "\n",
    "Входной Слой: 4 элемента, Выходной Слой: 1 элемент, Скрытый Слой: 6 элементов.\n",
    "\n",
    "![](../../img/lec07_25.png)\n",
    "![](../../img/lec07_26.png)\n",
    "\n",
    "**Прямое Распространение (1)— Из Входного Слоя в Выходной Слой**\n",
    "\n",
    "![](../../img/lec07_27.png)\n",
    "![](../../img/lec07_28.png)\n",
    "\n",
    "**Прямое Распространение (2) — Из Скрытого Слоя в Выходной Слой**\n",
    "\n",
    "![](../../img/lec07_29.png)\n",
    "\n",
    "В итоге, мы можем получить конечное значение из Выходного Слоя и использовать его для прогнозирования. \n",
    "\n",
    "* Если итоговое значение ≥ 0.5, мы прогнозируем метку y=1.\n",
    "* Если нет, мы прогнозируем label y=0.\n",
    "\n",
    "\n",
    "![](../../img/lec07_30.png)\n",
    "\n",
    "### **Задача Многоклассовой Классификации**\n",
    "\n",
    "В задаче многоклассовой классификации более одного элемента в Выходном Слое. Каждый элемент отражает **насколько точно данные относятся к категории.** В данной ситуации мы можем применить стратегию один-против-всех, чтобы определить результат прогнозирования. Заметьте, что Прямое Распространение - то же самое, что и задача бинарной классификации. \n",
    "\n",
    "![](../../img/lec07_31.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Функция Потерь (Loss Function)\n",
    "\n",
    "Функцию Потерь также называют Функция Затрат, но именно это название чаще всего встречается в NN.\n",
    "\n",
    "Цель Функции Потерь в оценке ошибки, которую совершила модель. Здесь мы показываем формулу, полученную из Функции Потерь. Заметьте, что узел NN - это логистическая единица с Сигмоидальной (Логистической) Функцией Активации.\n",
    "\n",
    "![](../../img/lec07_32.png)\n",
    "\n",
    "## Обратное Распространение (Backward Propagation)\n",
    "\n",
    "Данная операция применима для **минимизации Функции Потерь**, прямо как метод градиентного спуска, использованный в предыдущей заметке. \n",
    "\n",
    "Способ, использованный здесь схож с Градиентным Спуском и включает лишь 2 шага:\n",
    "\n",
    "1. Рассчитать частные производные от $J(\\theta)$ или $J(W)$\n",
    "2. Обновить каждый элемент матрицы весов **$\\theta$**.\n",
    "\n",
    "![](../../img/lec07_33.png)\n",
    "\n",
    "## Интуитивное понимание\n",
    "\n",
    "Давайте визуализируем процедуру, используя результат нахождения частной производной производной от $J(\\theta)$\n",
    "\n",
    "Давайте начнем с другого примера, принимая матрицы весов **$\\theta$** (обозначены **$W$**) уже иницилизированными. \n",
    "Наша цель в оптимизации **$J(\\theta)$** и обновлении матриц весов **$\\theta$**.\n",
    "\n",
    "Заметьте - мы может инициализировать матрицы весов Θ случайным образом, если у нас нет предварительного знания о задаче.\n",
    "\n",
    "![](../../img/lec07_34.png)\n",
    "\n",
    "После расчетов Calculus, мы можем получить частную производную от **$J(\\theta)$**\n",
    " \n",
    "\n",
    "![](../../img/lec07_35.png)\n",
    "![](../../img/lec07_36.png)\n",
    "\n",
    "Давайте углубимся в значение **$\\delta$**- \n",
    "\n",
    "* **$\\delta^3$** ошибка слоя 3 (в этом примере - Выходной Слой). \n",
    "* **$\\delta^2$** ошибка слоя 2 (в этом примере - Скрытый Слой).           \n",
    "* **$\\delta^1$** не существует, поскольку слой 1 - Входной Слой.\n",
    "\n",
    "![](../../img/lec07_37.png)\n",
    "![](../../img/lec07_38.png)\n",
    "\n",
    "Теперь, имея результат частной произвоной от $J(\\theta)$, вы можете обновить матрицу весов Θ.\n",
    "\n",
    "![](../../img/lec07_39.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Подробнее о Частной Производной от $J(\\theta)$\n",
    "\n",
    "В данном разделе, мы дадим вам несколько подсказок о производных из формулировки выше.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](../../img/lec07_40.png)\n",
    "\n",
    "Для удобства, мы используем этот же пример и предполагаем, что у нас есть только одни данные. Следовательно, функция потерь $J(\\theta)$ проще, чем общая форма. \n",
    "![](../../img/lec07_41.png)\n",
    "\n",
    "Для того, чтобы выполнить Обратное Распространение, сперва необходимо выполнить  Прямое Распространение.\n",
    "\n",
    "![](../../img/lec07_42.png)\n",
    "\n",
    "Теперь мы можем получить Частную Производную J(Θ). Здесь мы показываем Частные Производные от двух элементов W¹ и W² соответственно. Если хотите, вы можете решить ее с подсказками.\n",
    "\n",
    "\n",
    "![](../../img/lec07_43.png)\n",
    "\n",
    "![](../../img/lec07_44.png)\n",
    "\n",
    "Наконец, вы можете доказать формулу частной производной от J(Θ), если вы проследуете алгоритму выше. Если вы не знакомы с Calculus, не волнуйтесь, мы дали вам достаточно идей в разделе ***Инутитивное понимание.***\n",
    "\n",
    "## Как реализовать NN на практике? \n",
    "\n",
    "<a href=\"https://medium.com/@qempsil0914/implement-neural-network-without-using-deep-learning-libraries-step-by-step-tutorial-python3-e2aa4e5766d1\" class=\"bb cn kh ki kj kk\">Здесь ссылка на задание</a>,\n",
    "мы реализуем NN без использования библиотеки deep learning. Надеемся, что туториал поможет вам в более полном понимании архитектуры NN и методов Прямого и Обратного Распространения.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылки: https://medium.com/@qempsil0914/courseras-machine-learning-notes-week5-neural-network-lost-function-forward-and-backward-8b293401e4dc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
